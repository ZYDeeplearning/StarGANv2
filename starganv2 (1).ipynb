{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from pathlib import Path\nfrom itertools import chain\nimport os\nimport random\n\nfrom munch import Munch\nfrom PIL import Image\nimport numpy as np\n\nimport torch\nfrom torch.utils import data\nfrom torch.utils.data.sampler import WeightedRandomSampler\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\n\n\ndef listdir(dname):\n    fnames = list(chain(*[list(Path(dname).rglob('*.' + ext))\n                          for ext in ['png', 'jpg', 'jpeg', 'JPG']]))\n    return fnames\n\n\nclass DefaultDataset(data.Dataset):\n    def __init__(self, root, transform=None):\n        self.samples = listdir(root)\n        self.samples.sort()\n        self.transform = transform\n        self.targets = None\n\n    def __getitem__(self, index):\n        fname = self.samples[index]\n        img = Image.open(fname).convert('RGB')\n        if self.transform is not None:\n            img = self.transform(img)\n        return img\n\n    def __len__(self):\n        return len(self.samples)\n\n\nclass ReferenceDataset(data.Dataset):\n    def __init__(self, root, transform=None):\n        self.samples, self.targets = self._make_dataset(root)\n        self.transform = transform\n\n    def _make_dataset(self, root):\n        domains = os.listdir(root)\n        fnames, fnames2, labels = [], [], []\n        for idx, domain in enumerate(sorted(domains)):\n            class_dir = os.path.join(root, domain)\n            cls_fnames = listdir(class_dir)\n            fnames += cls_fnames\n            fnames2 += random.sample(cls_fnames, len(cls_fnames))\n            labels += [idx] * len(cls_fnames)\n        return list(zip(fnames, fnames2)), labels\n\n    def __getitem__(self, index):\n        fname, fname2 = self.samples[index]\n        label = self.targets[index]\n        img = Image.open(fname).convert('RGB')\n        img2 = Image.open(fname2).convert('RGB')\n        if self.transform is not None:\n            img = self.transform(img)\n            img2 = self.transform(img2)\n        return img, img2, label\n\n    def __len__(self):\n        return len(self.targets)\n\n\ndef _make_balanced_sampler(labels):\n    class_counts = np.bincount(labels)\n    class_weights = 1. / class_counts\n    weights = class_weights[labels]\n    return WeightedRandomSampler(weights, len(weights))\n\n\ndef get_train_loader(root, which='source', img_size=256,\n                     batch_size=8, prob=0.5, num_workers=4):\n    print('Preparing DataLoader to fetch %s images '\n          'during the training phase...' % which)\n\n    crop = transforms.RandomResizedCrop(\n        img_size, scale=[0.8, 1.0], ratio=[0.9, 1.1])\n    rand_crop = transforms.Lambda(\n        lambda x: crop(x) if random.random() < prob else x)\n\n    transform = transforms.Compose([\n        rand_crop,\n        transforms.Resize([img_size, img_size]),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                             std=[0.5, 0.5, 0.5]),\n    ])\n\n    if which == 'source':\n        dataset = ImageFolder(root, transform)\n    elif which == 'reference':\n        dataset = ReferenceDataset(root, transform)\n    else:\n        raise NotImplementedError\n\n    sampler = _make_balanced_sampler(dataset.targets)\n    return data.DataLoader(dataset=dataset,\n                           batch_size=batch_size,\n                           sampler=sampler,\n                           num_workers=num_workers,\n                           pin_memory=True,\n                           drop_last=True)\n\n\ndef get_eval_loader(root, img_size=256, batch_size=32,\n                    imagenet_normalize=True, shuffle=True,\n                    num_workers=4, drop_last=False):\n    print('Preparing DataLoader for the evaluation phase...')\n    if imagenet_normalize:\n        height, width = 299, 299\n        mean = [0.485, 0.456, 0.406]\n        std = [0.229, 0.224, 0.225]\n    else:\n        height, width = img_size, img_size\n        mean = [0.5, 0.5, 0.5]\n        std = [0.5, 0.5, 0.5]\n\n    transform = transforms.Compose([\n        transforms.Resize([img_size, img_size]),\n        transforms.Resize([height, width]),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=mean, std=std)\n    ])\n\n    dataset = DefaultDataset(root, transform=transform)\n    return data.DataLoader(dataset=dataset,\n                           batch_size=batch_size,\n                           shuffle=shuffle,\n                           num_workers=num_workers,\n                           pin_memory=True,\n                           drop_last=drop_last)\n\n\ndef get_test_loader(root, img_size=256, batch_size=32,\n                    shuffle=True, num_workers=4):\n    print('Preparing DataLoader for the generation phase...')\n    transform = transforms.Compose([\n        transforms.Resize([img_size, img_size]),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                             std=[0.5, 0.5, 0.5]),\n    ])\n\n    dataset = ImageFolder(root, transform)\n    return data.DataLoader(dataset=dataset,\n                           batch_size=batch_size,\n                           shuffle=shuffle,\n                           num_workers=num_workers,\n                           pin_memory=True)\n\n\nclass InputFetcher:\n    def __init__(self, loader, loader_ref=None, latent_dim=16, mode=''):\n        self.loader = loader\n        self.loader_ref = loader_ref\n        self.latent_dim = latent_dim\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.mode = mode\n\n    def _fetch_inputs(self):\n        try:\n            x, y = next(self.iter)\n        except (AttributeError, StopIteration):\n            self.iter = iter(self.loader)\n            x, y = next(self.iter)\n        return x, y\n\n    def _fetch_refs(self):\n        try:\n            x, x2, y = next(self.iter_ref)\n        except (AttributeError, StopIteration):\n            self.iter_ref = iter(self.loader_ref)\n            x, x2, y = next(self.iter_ref)\n        return x, x2, y\n\n    def __next__(self):\n        x, y = self._fetch_inputs()\n        if self.mode == 'train':\n            x_ref, x_ref2, y_ref = self._fetch_refs()\n            z_trg = torch.randn(x.size(0), self.latent_dim)\n            z_trg2 = torch.randn(x.size(0), self.latent_dim)\n            inputs = Munch(x_src=x, y_src=y, y_ref=y_ref,\n                           x_ref=x_ref, x_ref2=x_ref2,\n                           z_trg=z_trg, z_trg2=z_trg2)\n        elif self.mode == 'val':\n            x_ref, y_ref = self._fetch_inputs()\n            inputs = Munch(x_src=x, y_src=y,\n                           x_ref=x_ref, y_ref=y_ref)\n        elif self.mode == 'test':\n            inputs = Munch(x=x, y=y)\n        else:\n            raise NotImplementedError\n\n        return Munch({k: v.to(self.device)\n                      for k, v in inputs.items()})","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-04T10:05:16.416909Z","iopub.execute_input":"2023-01-04T10:05:16.417704Z","iopub.status.idle":"2023-01-04T10:05:18.872683Z","shell.execute_reply.started":"2023-01-04T10:05:16.417602Z","shell.execute_reply":"2023-01-04T10:05:18.870883Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install  ffmpeg-python==0.2.0 scikit-image==0.16.2","metadata":{"execution":{"iopub.status.busy":"2023-01-04T10:05:18.875592Z","iopub.execute_input":"2023-01-04T10:05:18.876831Z","iopub.status.idle":"2023-01-04T10:05:37.041591Z","shell.execute_reply.started":"2023-01-04T10:05:18.876747Z","shell.execute_reply":"2023-01-04T10:05:37.040288Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting ffmpeg-python==0.2.0\n  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\nCollecting scikit-image==0.16.2\n  Downloading scikit_image-0.16.2-cp37-cp37m-manylinux1_x86_64.whl (26.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from ffmpeg-python==0.2.0) (0.18.2)\nRequirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image==0.16.2) (2.5)\nRequirement already satisfied: pillow>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image==0.16.2) (9.1.1)\nRequirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image==0.16.2) (2.19.3)\nRequirement already satisfied: PyWavelets>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image==0.16.2) (1.3.0)\nRequirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image==0.16.2) (3.5.3)\nRequirement already satisfied: scipy>=0.19.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image==0.16.2) (1.7.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from imageio>=2.3.0->scikit-image==0.16.2) (1.21.6)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2) (4.33.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2) (1.4.3)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2) (0.11.0)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2) (3.0.9)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2) (21.3)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2) (2.8.2)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->scikit-image==0.16.2) (5.1.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2) (4.4.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2) (1.15.0)\nInstalling collected packages: ffmpeg-python, scikit-image\n  Attempting uninstall: scikit-image\n    Found existing installation: scikit-image 0.19.3\n    Uninstalling scikit-image-0.19.3:\n      Successfully uninstalled scikit-image-0.19.3\nSuccessfully installed ffmpeg-python-0.2.0 scikit-image-0.16.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"\nStarGAN v2\nCopyright (c) 2020-present NAVER Corp.\n\nThis work is licensed under the Creative Commons Attribution-NonCommercial\n4.0 International License. To view a copy of this license, visit\nhttp://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\nCreative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\"\"\"\n\nimport os\nfrom os.path import join as ospj\nimport json\nimport glob\nfrom shutil import copyfile\n\nfrom tqdm import tqdm\nimport ffmpeg\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.utils as vutils\n\n\ndef save_json(json_file, filename):\n    with open(filename, 'w') as f:\n        json.dump(json_file, f, indent=4, sort_keys=False)\n\n\ndef print_network(network, name):\n    num_params = 0\n    for p in network.parameters():\n        num_params += p.numel()\n    # print(network)\n    print(\"Number of parameters of %s: %i\" % (name, num_params))\n\n\ndef he_init(module):\n    if isinstance(module, nn.Conv2d):\n        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n        if module.bias is not None:\n            nn.init.constant_(module.bias, 0)\n    if isinstance(module, nn.Linear):\n        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n        if module.bias is not None:\n            nn.init.constant_(module.bias, 0)\n\n\ndef denormalize(x):\n    out = (x + 1) / 2\n    return out.clamp_(0, 1)\n\n\ndef save_image(x, ncol, filename):\n    x = denormalize(x)\n    vutils.save_image(x.cpu(), filename, nrow=ncol, padding=0)\n\n\n@torch.no_grad()\ndef translate_and_reconstruct(nets, args, x_src, y_src, x_ref, y_ref, filename):\n    N, C, H, W = x_src.size()\n    s_ref = nets.style_encoder(x_ref, y_ref)\n    masks = nets.fan.get_heatmap(x_src) if args.w_hpf > 0 else None\n    x_fake = nets.generator(x_src, s_ref, masks=masks)\n    s_src = nets.style_encoder(x_src, y_src)\n    masks = nets.fan.get_heatmap(x_fake) if args.w_hpf > 0 else None\n    x_rec = nets.generator(x_fake, s_src, masks=masks)\n    x_concat = [x_src, x_ref, x_fake, x_rec]\n    x_concat = torch.cat(x_concat, dim=0)\n    save_image(x_concat, N, filename)\n    del x_concat\n\n\n@torch.no_grad()\ndef translate_using_latent(nets, args, x_src, y_trg_list, z_trg_list, psi, filename):\n    N, C, H, W = x_src.size()\n    latent_dim = z_trg_list[0].size(1)\n    x_concat = [x_src]\n    masks = nets.fan.get_heatmap(x_src) if args.w_hpf > 0 else None\n\n    for i, y_trg in enumerate(y_trg_list):\n        z_many = torch.randn(10000, latent_dim).to(x_src.device)\n        y_many = torch.LongTensor(10000).to(x_src.device).fill_(y_trg[0])\n        s_many = nets.mapping_network(z_many, y_many)\n        s_avg = torch.mean(s_many, dim=0, keepdim=True)\n        s_avg = s_avg.repeat(N, 1)\n\n        for z_trg in z_trg_list:\n            s_trg = nets.mapping_network(z_trg, y_trg)\n            s_trg = torch.lerp(s_avg, s_trg, psi)\n            x_fake = nets.generator(x_src, s_trg, masks=masks)\n            x_concat += [x_fake]\n\n    x_concat = torch.cat(x_concat, dim=0)\n    save_image(x_concat, N, filename)\n\n\n@torch.no_grad()\ndef translate_using_reference(nets, args, x_src, x_ref, y_ref, filename):\n    N, C, H, W = x_src.size()\n    wb = torch.ones(1, C, H, W).to(x_src.device)\n    x_src_with_wb = torch.cat([wb, x_src], dim=0)\n\n    masks = nets.fan.get_heatmap(x_src) if args.w_hpf > 0 else None\n    s_ref = nets.style_encoder(x_ref, y_ref)\n    s_ref_list = s_ref.unsqueeze(1).repeat(1, N, 1)\n    x_concat = [x_src_with_wb]\n    for i, s_ref in enumerate(s_ref_list):\n        x_fake = nets.generator(x_src, s_ref, masks=masks)\n        x_fake_with_ref = torch.cat([x_ref[i:i+1], x_fake], dim=0)\n        x_concat += [x_fake_with_ref]\n\n    x_concat = torch.cat(x_concat, dim=0)\n    save_image(x_concat, N+1, filename)\n    del x_concat\n\n\n@torch.no_grad()\ndef debug_image(nets, args, inputs, step):\n    x_src, y_src = inputs.x_src, inputs.y_src\n    x_ref, y_ref = inputs.x_ref, inputs.y_ref\n\n    device = inputs.x_src.device\n    N = inputs.x_src.size(0)\n\n    # translate and reconstruct (reference-guided)\n    filename = ospj(args.sample_dir, '%06d_cycle_consistency.jpg' % (step))\n    translate_and_reconstruct(nets, args, x_src, y_src, x_ref, y_ref, filename)\n\n    # latent-guided image synthesis\n    y_trg_list = [torch.tensor(y).repeat(N).to(device)\n                  for y in range(min(args.num_domains, 5))]\n    z_trg_list = torch.randn(args.num_outs_per_domain, 1, args.latent_dim).repeat(1, N, 1).to(device)\n    for psi in [0.5, 0.7, 1.0]:\n        filename = ospj(args.sample_dir, '%06d_latent_psi_%.1f.jpg' % (step, psi))\n        translate_using_latent(nets, args, x_src, y_trg_list, z_trg_list, psi, filename)\n\n    # reference-guided image synthesis\n    filename = ospj(args.sample_dir, '%06d_reference.jpg' % (step))\n    translate_using_reference(nets, args, x_src, x_ref, y_ref, filename)\n\n\n# ======================= #\n# Video-related functions #\n# ======================= #\n\n\ndef sigmoid(x, w=1):\n    return 1. / (1 + np.exp(-w * x))\n\n\ndef get_alphas(start=-5, end=5, step=0.5, len_tail=10):\n    return [0] + [sigmoid(alpha) for alpha in np.arange(start, end, step)] + [1] * len_tail\n\n\ndef interpolate(nets, args, x_src, s_prev, s_next):\n    ''' returns T x C x H x W '''\n    B = x_src.size(0)\n    frames = []\n    masks = nets.fan.get_heatmap(x_src) if args.w_hpf > 0 else None\n    alphas = get_alphas()\n\n    for alpha in alphas:\n        s_ref = torch.lerp(s_prev, s_next, alpha)\n        x_fake = nets.generator(x_src, s_ref, masks=masks)\n        entries = torch.cat([x_src.cpu(), x_fake.cpu()], dim=2)\n        frame = torchvision.utils.make_grid(entries, nrow=B, padding=0, pad_value=-1).unsqueeze(0)\n        frames.append(frame)\n    frames = torch.cat(frames)\n    return frames\n\n\ndef slide(entries, margin=32):\n    \"\"\"Returns a sliding reference window.\n    Args:\n        entries: a list containing two reference images, x_prev and x_next, \n                 both of which has a shape (1, 3, 256, 256)\n    Returns:\n        canvas: output slide of shape (num_frames, 3, 256*2, 256+margin)\n    \"\"\"\n    _, C, H, W = entries[0].shape\n    alphas = get_alphas()\n    T = len(alphas) # number of frames\n\n    canvas = - torch.ones((T, C, H*2, W + margin))\n    merged = torch.cat(entries, dim=2)  # (1, 3, 512, 256)\n    for t, alpha in enumerate(alphas):\n        top = int(H * (1 - alpha))  # top, bottom for canvas\n        bottom = H * 2\n        m_top = 0  # top, bottom for merged\n        m_bottom = 2 * H - top\n        canvas[t, :, top:bottom, :W] = merged[:, :, m_top:m_bottom, :]\n    return canvas\n\n\n@torch.no_grad()\ndef video_ref(nets, args, x_src, x_ref, y_ref, fname):\n    video = []\n    s_ref = nets.style_encoder(x_ref, y_ref)\n    s_prev = None\n    for data_next in tqdm(zip(x_ref, y_ref, s_ref), 'video_ref', len(x_ref)):\n        x_next, y_next, s_next = [d.unsqueeze(0) for d in data_next]\n        if s_prev is None:\n            x_prev, y_prev, s_prev = x_next, y_next, s_next\n            continue\n        if y_prev != y_next:\n            x_prev, y_prev, s_prev = x_next, y_next, s_next\n            continue\n\n        interpolated = interpolate(nets, args, x_src, s_prev, s_next)\n        entries = [x_prev, x_next]\n        slided = slide(entries)  # (T, C, 256*2, 256)\n        frames = torch.cat([slided, interpolated], dim=3).cpu()  # (T, C, 256*2, 256*(batch+1))\n        video.append(frames)\n        x_prev, y_prev, s_prev = x_next, y_next, s_next\n\n    # append last frame 10 time\n    for _ in range(10):\n        video.append(frames[-1:])\n    video = tensor2ndarray255(torch.cat(video))\n    save_video(fname, video)\n\n\n@torch.no_grad()\ndef video_latent(nets, args, x_src, y_list, z_list, psi, fname):\n    latent_dim = z_list[0].size(1)\n    s_list = []\n    for i, y_trg in enumerate(y_list):\n        z_many = torch.randn(10000, latent_dim).to(x_src.device)\n        y_many = torch.LongTensor(10000).to(x_src.device).fill_(y_trg[0])\n        s_many = nets.mapping_network(z_many, y_many)\n        s_avg = torch.mean(s_many, dim=0, keepdim=True)\n        s_avg = s_avg.repeat(x_src.size(0), 1)\n\n        for z_trg in z_list:\n            s_trg = nets.mapping_network(z_trg, y_trg)\n            s_trg = torch.lerp(s_avg, s_trg, psi)\n            s_list.append(s_trg)\n\n    s_prev = None\n    video = []\n    # fetch reference images\n    for idx_ref, s_next in enumerate(tqdm(s_list, 'video_latent', len(s_list))):\n        if s_prev is None:\n            s_prev = s_next\n            continue\n        if idx_ref % len(z_list) == 0:\n            s_prev = s_next\n            continue\n        frames = interpolate(nets, args, x_src, s_prev, s_next).cpu()\n        video.append(frames)\n        s_prev = s_next\n    for _ in range(10):\n        video.append(frames[-1:])\n    video = tensor2ndarray255(torch.cat(video))\n    save_video(fname, video)\n\n\ndef save_video(fname, images, output_fps=30, vcodec='libx264', filters=''):\n    assert isinstance(images, np.ndarray), \"images should be np.array: NHWC\"\n    num_frames, height, width, channels = images.shape\n    stream = ffmpeg.input('pipe:', format='rawvideo', \n                          pix_fmt='rgb24', s='{}x{}'.format(width, height))\n    stream = ffmpeg.filter(stream, 'setpts', '2*PTS')  # 2*PTS is for slower playback\n    stream = ffmpeg.output(stream, fname, pix_fmt='yuv420p', vcodec=vcodec, r=output_fps)\n    stream = ffmpeg.overwrite_output(stream)\n    process = ffmpeg.run_async(stream, pipe_stdin=True)\n    for frame in tqdm(images, desc='writing video to %s' % fname):\n        process.stdin.write(frame.astype(np.uint8).tobytes())\n    process.stdin.close()\n    process.wait()\n\n\ndef tensor2ndarray255(images):\n    images = torch.clamp(images * 0.5 + 0.5, 0, 1)\n    return images.cpu().numpy().transpose(0, 2, 3, 1) * 255","metadata":{"execution":{"iopub.status.busy":"2023-01-04T10:05:37.043855Z","iopub.execute_input":"2023-01-04T10:05:37.044267Z","iopub.status.idle":"2023-01-04T10:05:37.139998Z","shell.execute_reply.started":"2023-01-04T10:05:37.044227Z","shell.execute_reply":"2023-01-04T10:05:37.138996Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\n\nfrom collections import namedtuple\nfrom copy import deepcopy\nfrom functools import partial\n\nfrom munch import Munch\nimport numpy as np\nimport cv2\nfrom skimage.filters import gaussian\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef get_preds_fromhm(hm):\n    max, idx = torch.max(\n        hm.view(hm.size(0), hm.size(1), hm.size(2) * hm.size(3)), 2)\n    idx += 1\n    preds = idx.view(idx.size(0), idx.size(1), 1).repeat(1, 1, 2).float()\n    preds[..., 0].apply_(lambda x: (x - 1) % hm.size(3) + 1)\n    preds[..., 1].add_(-1).div_(hm.size(2)).floor_().add_(1)\n\n    for i in range(preds.size(0)):\n        for j in range(preds.size(1)):\n            hm_ = hm[i, j, :]\n            pX, pY = int(preds[i, j, 0]) - 1, int(preds[i, j, 1]) - 1\n            if pX > 0 and pX < 63 and pY > 0 and pY < 63:\n                diff = torch.FloatTensor(\n                    [hm_[pY, pX + 1] - hm_[pY, pX - 1],\n                     hm_[pY + 1, pX] - hm_[pY - 1, pX]])\n                preds[i, j].add_(diff.sign_().mul_(.25))\n\n    preds.add_(-0.5)\n    return preds\n\n\nclass HourGlass(nn.Module):\n    def __init__(self, num_modules, depth, num_features, first_one=False):\n        super(HourGlass, self).__init__()\n        self.num_modules = num_modules\n        self.depth = depth\n        self.features = num_features\n        self.coordconv = CoordConvTh(64, 64, True, True, 256, first_one,\n                                     out_channels=256,\n                                     kernel_size=1, stride=1, padding=0)\n        self._generate_network(self.depth)\n\n    def _generate_network(self, level):\n        self.add_module('b1_' + str(level), ConvBlock(256, 256))\n        self.add_module('b2_' + str(level), ConvBlock(256, 256))\n        if level > 1:\n            self._generate_network(level - 1)\n        else:\n            self.add_module('b2_plus_' + str(level), ConvBlock(256, 256))\n        self.add_module('b3_' + str(level), ConvBlock(256, 256))\n\n    def _forward(self, level, inp):\n        up1 = inp\n        up1 = self._modules['b1_' + str(level)](up1)\n        low1 = F.avg_pool2d(inp, 2, stride=2)\n        low1 = self._modules['b2_' + str(level)](low1)\n\n        if level > 1:\n            low2 = self._forward(level - 1, low1)\n        else:\n            low2 = low1\n            low2 = self._modules['b2_plus_' + str(level)](low2)\n        low3 = low2\n        low3 = self._modules['b3_' + str(level)](low3)\n        up2 = F.interpolate(low3, scale_factor=2, mode='nearest')\n\n        return up1 + up2\n\n    def forward(self, x, heatmap):\n        x, last_channel = self.coordconv(x, heatmap)\n        return self._forward(self.depth, x), last_channel\n\n\nclass AddCoordsTh(nn.Module):\n    def __init__(self, height=64, width=64, with_r=False, with_boundary=False):\n        super(AddCoordsTh, self).__init__()\n        self.with_r = with_r\n        self.with_boundary = with_boundary\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n        with torch.no_grad():\n            x_coords = torch.arange(height).unsqueeze(1).expand(height, width).float()\n            y_coords = torch.arange(width).unsqueeze(0).expand(height, width).float()\n            x_coords = (x_coords / (height - 1)) * 2 - 1\n            y_coords = (y_coords / (width - 1)) * 2 - 1\n            coords = torch.stack([x_coords, y_coords], dim=0)  # (2, height, width)\n\n            if self.with_r:\n                rr = torch.sqrt(torch.pow(x_coords, 2) + torch.pow(y_coords, 2))  # (height, width)\n                rr = (rr / torch.max(rr)).unsqueeze(0)\n                coords = torch.cat([coords, rr], dim=0)\n\n            self.coords = coords.unsqueeze(0).to(device)  # (1, 2 or 3, height, width)\n            self.x_coords = x_coords.to(device)\n            self.y_coords = y_coords.to(device)\n\n    def forward(self, x, heatmap=None):\n        \"\"\"\n        x: (batch, c, x_dim, y_dim)\n        \"\"\"\n        coords = self.coords.repeat(x.size(0), 1, 1, 1)\n\n        if self.with_boundary and heatmap is not None:\n            boundary_channel = torch.clamp(heatmap[:, -1:, :, :], 0.0, 1.0)\n            zero_tensor = torch.zeros_like(self.x_coords)\n            xx_boundary_channel = torch.where(boundary_channel > 0.05, self.x_coords, zero_tensor).to(zero_tensor.device)\n            yy_boundary_channel = torch.where(boundary_channel > 0.05, self.y_coords, zero_tensor).to(zero_tensor.device)\n            coords = torch.cat([coords, xx_boundary_channel, yy_boundary_channel], dim=1)\n\n        x_and_coords = torch.cat([x, coords], dim=1)\n        return x_and_coords\n\n\nclass CoordConvTh(nn.Module):\n    \"\"\"CoordConv layer as in the paper.\"\"\"\n    def __init__(self, height, width, with_r, with_boundary,\n                 in_channels, first_one=False, *args, **kwargs):\n        super(CoordConvTh, self).__init__()\n        self.addcoords = AddCoordsTh(height, width, with_r, with_boundary)\n        in_channels += 2\n        if with_r:\n            in_channels += 1\n        if with_boundary and not first_one:\n            in_channels += 2\n        self.conv = nn.Conv2d(in_channels=in_channels, *args, **kwargs)\n\n    def forward(self, input_tensor, heatmap=None):\n        ret = self.addcoords(input_tensor, heatmap)\n        last_channel = ret[:, -2:, :, :]\n        ret = self.conv(ret)\n        return ret, last_channel\n\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_planes, out_planes):\n        super(ConvBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        conv3x3 = partial(nn.Conv2d, kernel_size=3, stride=1, padding=1, bias=False, dilation=1)\n        self.conv1 = conv3x3(in_planes, int(out_planes / 2))\n        self.bn2 = nn.BatchNorm2d(int(out_planes / 2))\n        self.conv2 = conv3x3(int(out_planes / 2), int(out_planes / 4))\n        self.bn3 = nn.BatchNorm2d(int(out_planes / 4))\n        self.conv3 = conv3x3(int(out_planes / 4), int(out_planes / 4))\n\n        self.downsample = None\n        if in_planes != out_planes:\n            self.downsample = nn.Sequential(nn.BatchNorm2d(in_planes),\n                                            nn.ReLU(True),\n                                            nn.Conv2d(in_planes, out_planes, 1, 1, bias=False))\n\n    def forward(self, x):\n        residual = x\n\n        out1 = self.bn1(x)\n        out1 = F.relu(out1, True)\n        out1 = self.conv1(out1)\n\n        out2 = self.bn2(out1)\n        out2 = F.relu(out2, True)\n        out2 = self.conv2(out2)\n\n        out3 = self.bn3(out2)\n        out3 = F.relu(out3, True)\n        out3 = self.conv3(out3)\n\n        out3 = torch.cat((out1, out2, out3), 1)\n        if self.downsample is not None:\n            residual = self.downsample(residual)\n        out3 += residual\n        return out3\n\n\nclass FAN(nn.Module):\n    def __init__(self, num_modules=1, end_relu=False, num_landmarks=98, fname_pretrained=None):\n        super(FAN, self).__init__()\n        self.num_modules = num_modules\n        self.end_relu = end_relu\n\n        # Base part\n        self.conv1 = CoordConvTh(256, 256, True, False,\n                                 in_channels=3, out_channels=64,\n                                 kernel_size=7, stride=2, padding=3)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.conv2 = ConvBlock(64, 128)\n        self.conv3 = ConvBlock(128, 128)\n        self.conv4 = ConvBlock(128, 256)\n\n        # Stacking part\n        self.add_module('m0', HourGlass(1, 4, 256, first_one=True))\n        self.add_module('top_m_0', ConvBlock(256, 256))\n        self.add_module('conv_last0', nn.Conv2d(256, 256, 1, 1, 0))\n        self.add_module('bn_end0', nn.BatchNorm2d(256))\n        self.add_module('l0', nn.Conv2d(256, num_landmarks+1, 1, 1, 0))\n\n#         if fname_pretrained is not None:\n#             self.load_pretrained_weights(fname_pretrained)\n\n    def load_pretrained_weights(self, fname):\n        if torch.cuda.is_available():\n            checkpoint = torch.load(fname)\n        else:\n            checkpoint = torch.load(fname, map_location=torch.device('cpu'))\n        model_weights = self.state_dict()\n        model_weights.update({k: v for k, v in checkpoint['state_dict'].items()\n                              if k in model_weights})\n        self.load_state_dict(model_weights)\n\n    def forward(self, x):\n        x, _ = self.conv1(x)\n        x = F.relu(self.bn1(x), True)\n        x = F.avg_pool2d(self.conv2(x), 2, stride=2)\n        x = self.conv3(x)\n        x = self.conv4(x)\n\n        outputs = []\n        boundary_channels = []\n        tmp_out = None\n        ll, boundary_channel = self._modules['m0'](x, tmp_out)\n        ll = self._modules['top_m_0'](ll)\n        ll = F.relu(self._modules['bn_end0']\n                    (self._modules['conv_last0'](ll)), True)\n\n        # Predict heatmaps\n        tmp_out = self._modules['l0'](ll)\n        if self.end_relu:\n            tmp_out = F.relu(tmp_out)  # HACK: Added relu\n        outputs.append(tmp_out)\n        boundary_channels.append(boundary_channel)\n        return outputs, boundary_channels\n\n    @torch.no_grad()\n    def get_heatmap(self, x, b_preprocess=True):\n        ''' outputs 0-1 normalized heatmap '''\n        x = F.interpolate(x, size=256, mode='bilinear')\n        x_01 = x*0.5 + 0.5\n        outputs, _ = self(x_01)\n        heatmaps = outputs[-1][:, :-1, :, :]\n        scale_factor = x.size(2) // heatmaps.size(2)\n        if b_preprocess:\n            heatmaps = F.interpolate(heatmaps, scale_factor=scale_factor,\n                                     mode='bilinear', align_corners=True)\n            heatmaps = preprocess(heatmaps)\n        return heatmaps\n\n    @torch.no_grad()\n    def get_landmark(self, x):\n        ''' outputs landmarks of x.shape '''\n        heatmaps = self.get_heatmap(x, b_preprocess=False)\n        landmarks = []\n        for i in range(x.size(0)):\n            pred_landmarks = get_preds_fromhm(heatmaps[i].cpu().unsqueeze(0))\n            landmarks.append(pred_landmarks)\n        scale_factor = x.size(2) // heatmaps.size(2)\n        landmarks = torch.cat(landmarks) * scale_factor\n        return landmarks\n\n\n# ========================== #\n#   Align related functions  #\n# ========================== #\n\n\ndef tensor2numpy255(tensor):\n    \"\"\"Converts torch tensor to numpy array.\"\"\"\n    return ((tensor.permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5) * 255).astype('uint8')\n\n\ndef np2tensor(image):\n    \"\"\"Converts numpy array to torch tensor.\"\"\"\n    return torch.FloatTensor(image).permute(2, 0, 1) / 255 * 2 - 1\n\n\nclass FaceAligner():\n    def __init__(self, fname_wing, fname_celeba_mean, output_size):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.fan = FAN(fname_pretrained=fname_wing).to(self.device).eval()\n        scale = output_size // 256\n        self.CELEB_REF = np.float32(np.load(fname_celeba_mean)['mean']) * scale\n        self.xaxis_ref = landmarks2xaxis(self.CELEB_REF)\n        self.output_size = output_size\n\n    def align(self, imgs, output_size=256):\n        ''' imgs = torch.CUDATensor of BCHW '''\n        imgs = imgs.to(self.device)\n        landmarkss = self.fan.get_landmark(imgs).cpu().numpy()\n        for i, (img, landmarks) in enumerate(zip(imgs, landmarkss)):\n            img_np = tensor2numpy255(img)\n            img_np, landmarks = pad_mirror(img_np, landmarks)\n            transform = self.landmarks2mat(landmarks)\n            rows, cols, _ = img_np.shape\n            rows = max(rows, self.output_size)\n            cols = max(cols, self.output_size)\n            aligned = cv2.warpPerspective(img_np, transform, (cols, rows), flags=cv2.INTER_LANCZOS4)\n            imgs[i] = np2tensor(aligned[:self.output_size, :self.output_size, :])\n        return imgs\n\n    def landmarks2mat(self, landmarks):\n        T_origin = points2T(landmarks, 'from')\n        xaxis_src = landmarks2xaxis(landmarks)\n        R = vecs2R(xaxis_src, self.xaxis_ref)\n        S = landmarks2S(landmarks, self.CELEB_REF)\n        T_ref = points2T(self.CELEB_REF, 'to')\n        matrix = np.dot(T_ref, np.dot(S, np.dot(R, T_origin)))\n        return matrix\n\n\ndef points2T(point, direction):\n    point_mean = point.mean(axis=0)\n    T = np.eye(3)\n    coef = -1 if direction == 'from' else 1\n    T[:2, 2] = coef * point_mean\n    return T\n\n\ndef landmarks2eyes(landmarks):\n    idx_left = np.array(list(range(60, 67+1)) + [96])\n    idx_right = np.array(list(range(68, 75+1)) + [97])\n    left = landmarks[idx_left]\n    right = landmarks[idx_right]\n    return left.mean(axis=0), right.mean(axis=0)\n\n\ndef landmarks2mouthends(landmarks):\n    left = landmarks[76]\n    right = landmarks[82]\n    return left, right\n\n\ndef rotate90(vec):\n    x, y = vec\n    return np.array([y, -x])\n\n\ndef landmarks2xaxis(landmarks):\n    eye_left, eye_right = landmarks2eyes(landmarks)\n    mouth_left, mouth_right = landmarks2mouthends(landmarks)\n    xp = eye_right - eye_left  # x' in pggan\n    eye_center = (eye_left + eye_right) * 0.5\n    mouth_center = (mouth_left + mouth_right) * 0.5\n    yp = eye_center - mouth_center\n    xaxis = xp - rotate90(yp)\n    return xaxis / np.linalg.norm(xaxis)\n\n\ndef vecs2R(vec_x, vec_y):\n    vec_x = vec_x / np.linalg.norm(vec_x)\n    vec_y = vec_y / np.linalg.norm(vec_y)\n    c = np.dot(vec_x, vec_y)\n    s = np.sqrt(1 - c * c) * np.sign(np.cross(vec_x, vec_y))\n    R = np.array(((c, -s, 0), (s, c, 0), (0, 0, 1)))\n    return R\n\n\ndef landmarks2S(x, y):\n    x_mean = x.mean(axis=0).squeeze()\n    y_mean = y.mean(axis=0).squeeze()\n    # vectors = mean -> each point\n    x_vectors = x - x_mean\n    y_vectors = y - y_mean\n\n    x_norms = np.linalg.norm(x_vectors, axis=1)\n    y_norms = np.linalg.norm(y_vectors, axis=1)\n\n    indices = [96, 97, 76, 82]  # indices for eyes, lips\n    scale = (y_norms / x_norms)[indices].mean()\n\n    S = np.eye(3)\n    S[0, 0] = S[1, 1] = scale\n    return S\n\n\ndef pad_mirror(img, landmarks):\n    H, W, _ = img.shape\n    img = np.pad(img, ((H//2, H//2), (W//2, W//2), (0, 0)), 'reflect')\n    small_blurred = gaussian(cv2.resize(img, (W, H)), H//100, multichannel=True)\n    blurred = cv2.resize(small_blurred, (W * 2, H * 2)) * 255\n\n    H, W, _ = img.shape\n    coords = np.meshgrid(np.arange(H), np.arange(W), indexing=\"ij\")\n    weight_y = np.clip(coords[0] / (H//4), 0, 1)\n    weight_x = np.clip(coords[1] / (H//4), 0, 1)\n    weight_y = np.minimum(weight_y, np.flip(weight_y, axis=0))\n    weight_x = np.minimum(weight_x, np.flip(weight_x, axis=1))\n    weight = np.expand_dims(np.minimum(weight_y, weight_x), 2)**4\n    img = img * weight + blurred * (1 - weight)\n    landmarks += np.array([W//4, H//4])\n    return img, landmarks\n\n\ndef align_faces(args, input_dir, output_dir):\n    import os\n    from torchvision import transforms\n    from PIL import Image\n    from core.utils import save_image\n\n    aligner = FaceAligner(args.wing_path, args.lm_path, args.img_size)\n    transform = transforms.Compose([\n        transforms.Resize((args.img_size, args.img_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                             std=[0.5, 0.5, 0.5]),\n    ])\n\n    fnames = os.listdir(input_dir)\n    os.makedirs(output_dir, exist_ok=True)\n    fnames.sort()\n    for fname in fnames:\n        image = Image.open(os.path.join(input_dir, fname)).convert('RGB')\n        x = transform(image).unsqueeze(0)\n        x_aligned = aligner.align(x)\n        save_image(x_aligned, 1, filename=os.path.join(output_dir, fname))\n        print('Saved the aligned image to %s...' % fname)\n\n\n# ========================== #\n#   Mask related functions   #\n# ========================== #\n\n\ndef normalize(x, eps=1e-6):\n    \"\"\"Apply min-max normalization.\"\"\"\n    x = x.contiguous()\n    N, C, H, W = x.size()\n    x_ = x.view(N*C, -1)\n    max_val = torch.max(x_, dim=1, keepdim=True)[0]\n    min_val = torch.min(x_, dim=1, keepdim=True)[0]\n    x_ = (x_ - min_val) / (max_val - min_val + eps)\n    out = x_.view(N, C, H, W)\n    return out\n\n\ndef truncate(x, thres=0.1):\n    \"\"\"Remove small values in heatmaps.\"\"\"\n    return torch.where(x < thres, torch.zeros_like(x), x)\n\n\ndef resize(x, p=2):\n    \"\"\"Resize heatmaps.\"\"\"\n    return x**p\n\n\ndef shift(x, N):\n    \"\"\"Shift N pixels up or down.\"\"\"\n    up = N >= 0\n    N = abs(N)\n    _, _, H, W = x.size()\n    head = torch.arange(N)\n    tail = torch.arange(H-N)\n\n    if up:\n        head = torch.arange(H-N)+N\n        tail = torch.arange(N)\n    else:\n        head = torch.arange(N) + (H-N)\n        tail = torch.arange(H-N)\n\n    # permutation indices\n    perm = torch.cat([head, tail]).to(x.device)\n    out = x[:, :, perm, :]\n    return out\n\n\nIDXPAIR = namedtuple('IDXPAIR', 'start end')\nindex_map = Munch(chin=IDXPAIR(0 + 8, 33 - 8),\n                  eyebrows=IDXPAIR(33, 51),\n                  eyebrowsedges=IDXPAIR(33, 46),\n                  nose=IDXPAIR(51, 55),\n                  nostrils=IDXPAIR(55, 60),\n                  eyes=IDXPAIR(60, 76),\n                  lipedges=IDXPAIR(76, 82),\n                  lipupper=IDXPAIR(77, 82),\n                  liplower=IDXPAIR(83, 88),\n                  lipinner=IDXPAIR(88, 96))\nOPPAIR = namedtuple('OPPAIR', 'shift resize')\n\n\ndef preprocess(x):\n    \"\"\"Preprocess 98-dimensional heatmaps.\"\"\"\n    N, C, H, W = x.size()\n    x = truncate(x)\n    x = normalize(x)\n\n    sw = H // 256\n    operations = Munch(chin=OPPAIR(0, 3),\n                       eyebrows=OPPAIR(-7*sw, 2),\n                       nostrils=OPPAIR(8*sw, 4),\n                       lipupper=OPPAIR(-8*sw, 4),\n                       liplower=OPPAIR(8*sw, 4),\n                       lipinner=OPPAIR(-2*sw, 3))\n\n    for part, ops in operations.items():\n        start, end = index_map[part]\n        x[:, start:end] = resize(shift(x[:, start:end], ops.shift), ops.resize)\n\n    zero_out = torch.cat([torch.arange(0, index_map.chin.start),\n                          torch.arange(index_map.chin.end, 33),\n                          torch.LongTensor([index_map.eyebrowsedges.start,\n                                            index_map.eyebrowsedges.end,\n                                            index_map.lipedges.start,\n                                            index_map.lipedges.end])])\n    x[:, zero_out] = 0\n\n    start, end = index_map.nose\n    x[:, start+1:end] = shift(x[:, start+1:end], 4*sw)\n    x[:, start:end] = resize(x[:, start:end], 1)\n\n    start, end = index_map.eyes\n    x[:, start:end] = resize(x[:, start:end], 1)\n    x[:, start:end] = resize(shift(x[:, start:end], -8), 3) + \\\n        shift(x[:, start:end], -24)\n\n    # Second-level mask\n    x2 = deepcopy(x)\n    x2[:, index_map.chin.start:index_map.chin.end] = 0  # start:end was 0:33\n    x2[:, index_map.lipedges.start:index_map.lipinner.end] = 0  # start:end was 76:96\n    x2[:, index_map.eyebrows.start:index_map.eyebrows.end] = 0  # start:end was 33:51\n\n    x = torch.sum(x, dim=1, keepdim=True)  # (N, 1, H, W)\n    x2 = torch.sum(x2, dim=1, keepdim=True)  # mask without faceline and mouth\n\n    x[x != x] = 0  # set nan to zero\n    x2[x != x] = 0  # set nan to zero\n    return x.clamp_(0, 1), x2.clamp_(0, 1)\n\n\n\n\nclass CheckpointIO(object):\n    def __init__(self, fname_template, data_parallel=False, **kwargs):\n        os.makedirs(os.path.dirname(fname_template), exist_ok=True)\n        self.fname_template = fname_template\n        self.module_dict = kwargs\n        self.data_parallel = data_parallel\n\n    def register(self, **kwargs):\n        self.module_dict.update(kwargs)\n\n    def save(self, step):\n        fname = self.fname_template.format(step)\n        print('Saving checkpoint into %s...' % fname)\n        outdict = {}\n        for name, module in self.module_dict.items():\n            if self.data_parallel:\n                outdict[name] = module.module.state_dict()\n            else:\n                outdict[name] = module.state_dict()\n                        \n        torch.save(outdict, fname)\n\n    def load(self, step):\n        fname = self.fname_template.format(step)\n        assert os.path.exists(fname), fname + ' does not exist!'\n        print('Loading checkpoint from %s...' % fname)\n        if torch.cuda.is_available():\n            module_dict = torch.load(fname)\n        else:\n            module_dict = torch.load(fname, map_location=torch.device('cpu'))\n            \n        for name, module in self.module_dict.items():\n            if self.data_parallel:\n                module.module.load_state_dict(module_dict[name])\n            else:\n                module.load_state_dict(module_dict[name])\n","metadata":{"execution":{"iopub.status.busy":"2023-01-04T10:05:37.141716Z","iopub.execute_input":"2023-01-04T10:05:37.142100Z","iopub.status.idle":"2023-01-04T10:05:38.320716Z","shell.execute_reply.started":"2023-01-04T10:05:37.142068Z","shell.execute_reply":"2023-01-04T10:05:38.319542Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\n\nimport copy\nimport math\n\nfrom munch import Munch\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ResBlk(nn.Module):\n    def __init__(self, dim_in, dim_out, actv=nn.LeakyReLU(0.2),\n                 normalize=False, downsample=False):\n        super().__init__()\n        self.actv = actv\n        self.normalize = normalize\n        self.downsample = downsample\n        self.learned_sc = dim_in != dim_out\n        self._build_weights(dim_in, dim_out)\n\n    def _build_weights(self, dim_in, dim_out):\n        self.conv1 = nn.Conv2d(dim_in, dim_in, 3, 1, 1)\n        self.conv2 = nn.Conv2d(dim_in, dim_out, 3, 1, 1)\n        if self.normalize:\n            self.norm1 = nn.InstanceNorm2d(dim_in, affine=True)\n            self.norm2 = nn.InstanceNorm2d(dim_in, affine=True)\n        if self.learned_sc:\n            self.conv1x1 = nn.Conv2d(dim_in, dim_out, 1, 1, 0, bias=False)\n\n    def _shortcut(self, x):\n        if self.learned_sc:\n            x = self.conv1x1(x)\n        if self.downsample:\n            x = F.avg_pool2d(x, 2)\n        return x\n\n    def _residual(self, x):\n        if self.normalize:\n            x = self.norm1(x)\n        x = self.actv(x)\n        x = self.conv1(x)\n        if self.downsample:\n            x = F.avg_pool2d(x, 2)\n        if self.normalize:\n            x = self.norm2(x)\n        x = self.actv(x)\n        x = self.conv2(x)\n        return x\n\n    def forward(self, x):\n        x = self._shortcut(x) + self._residual(x)\n        return x / math.sqrt(2)  # unit variance\n\n\nclass AdaIN(nn.Module):\n    def __init__(self, style_dim, num_features):\n        super().__init__()\n        self.norm = nn.InstanceNorm2d(num_features, affine=False)\n        self.fc = nn.Linear(style_dim, num_features*2)\n\n    def forward(self, x, s):\n        h = self.fc(s)\n        h = h.view(h.size(0), h.size(1), 1, 1)\n        gamma, beta = torch.chunk(h, chunks=2, dim=1)\n        return (1 + gamma) * self.norm(x) + beta\n\n\nclass AdainResBlk(nn.Module):\n    def __init__(self, dim_in, dim_out, style_dim=64, w_hpf=0,\n                 actv=nn.LeakyReLU(0.2), upsample=False):\n        super().__init__()\n        self.w_hpf = w_hpf\n        self.actv = actv\n        self.upsample = upsample\n        self.learned_sc = dim_in != dim_out\n        self._build_weights(dim_in, dim_out, style_dim)\n\n    def _build_weights(self, dim_in, dim_out, style_dim=64):\n        self.conv1 = nn.Conv2d(dim_in, dim_out, 3, 1, 1)\n        self.conv2 = nn.Conv2d(dim_out, dim_out, 3, 1, 1)\n        self.norm1 = AdaIN(style_dim, dim_in)\n        self.norm2 = AdaIN(style_dim, dim_out)\n        if self.learned_sc:\n            self.conv1x1 = nn.Conv2d(dim_in, dim_out, 1, 1, 0, bias=False)\n\n    def _shortcut(self, x):\n        if self.upsample:\n            x = F.interpolate(x, scale_factor=2, mode='nearest')\n        if self.learned_sc:\n            x = self.conv1x1(x)\n        return x\n\n    def _residual(self, x, s):\n        x = self.norm1(x, s)\n        x = self.actv(x)\n        if self.upsample:\n            x = F.interpolate(x, scale_factor=2, mode='nearest')\n        x = self.conv1(x)\n        x = self.norm2(x, s)\n        x = self.actv(x)\n        x = self.conv2(x)\n        return x\n\n    def forward(self, x, s):\n        out = self._residual(x, s)\n        if self.w_hpf == 0:\n            out = (out + self._shortcut(x)) / math.sqrt(2)\n        return out\n\n\nclass HighPass(nn.Module):\n    def __init__(self, w_hpf, device):\n        super(HighPass, self).__init__()\n        self.register_buffer('filter',\n                             torch.tensor([[-1, -1, -1],\n                                           [-1, 8., -1],\n                                           [-1, -1, -1]]) / w_hpf)\n\n    def forward(self, x):\n        filter = self.filter.unsqueeze(0).unsqueeze(1).repeat(x.size(1), 1, 1, 1)\n        return F.conv2d(x, filter, padding=1, groups=x.size(1))\n\n\nclass Generator(nn.Module):\n    def __init__(self, img_size=256, style_dim=64, max_conv_dim=512, w_hpf=1):\n        super().__init__()\n        dim_in = 2**14 // img_size\n        self.img_size = img_size\n        self.from_rgb = nn.Conv2d(3, dim_in, 3, 1, 1)\n        self.encode = nn.ModuleList()\n        self.decode = nn.ModuleList()\n        self.to_rgb = nn.Sequential(\n            nn.InstanceNorm2d(dim_in, affine=True),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(dim_in, 3, 1, 1, 0))\n\n        # down/up-sampling blocks\n        repeat_num = int(np.log2(img_size)) - 4\n        if w_hpf > 0:\n            repeat_num += 1\n        for _ in range(repeat_num):\n            dim_out = min(dim_in*2, max_conv_dim)\n            self.encode.append(\n                ResBlk(dim_in, dim_out, normalize=True, downsample=True))\n            self.decode.insert(\n                0, AdainResBlk(dim_out, dim_in, style_dim,\n                               w_hpf=w_hpf, upsample=True))  # stack-like\n            dim_in = dim_out\n\n        # bottleneck blocks\n        for _ in range(2):\n            self.encode.append(\n                ResBlk(dim_out, dim_out, normalize=True))\n            self.decode.insert(\n                0, AdainResBlk(dim_out, dim_out, style_dim, w_hpf=w_hpf))\n\n        if w_hpf > 0:\n            device = torch.device(\n                'cuda' if torch.cuda.is_available() else 'cpu')\n            self.hpf = HighPass(w_hpf, device)\n\n    def forward(self, x, s, masks=None):\n        x = self.from_rgb(x)\n        cache = {}\n        for block in self.encode:\n            if (masks is not None) and (x.size(2) in [32, 64, 128]):\n                cache[x.size(2)] = x\n            x = block(x)\n        for block in self.decode:\n            x = block(x, s)\n            if (masks is not None) and (x.size(2) in [32, 64, 128]):\n                mask = masks[0] if x.size(2) in [32] else masks[1]\n                mask = F.interpolate(mask, size=x.size(2), mode='bilinear')\n                x = x + self.hpf(mask * cache[x.size(2)])\n        return self.to_rgb(x)\n\n\nclass MappingNetwork(nn.Module):\n    def __init__(self, latent_dim=16, style_dim=64, num_domains=2):\n        super().__init__()\n        layers = []\n        layers += [nn.Linear(latent_dim, 512)]\n        layers += [nn.ReLU()]\n        for _ in range(3):\n            layers += [nn.Linear(512, 512)]\n            layers += [nn.ReLU()]\n        self.shared = nn.Sequential(*layers)\n\n        self.unshared = nn.ModuleList()\n        for _ in range(num_domains):\n            self.unshared += [nn.Sequential(nn.Linear(512, 512),\n                                            nn.ReLU(),\n                                            nn.Linear(512, 512),\n                                            nn.ReLU(),\n                                            nn.Linear(512, 512),\n                                            nn.ReLU(),\n                                            nn.Linear(512, style_dim))]\n\n    def forward(self, z, y):\n        h = self.shared(z)\n        out = []\n        for layer in self.unshared:\n            out += [layer(h)]\n        out = torch.stack(out, dim=1)  # (batch, num_domains, style_dim)\n        idx = torch.LongTensor(range(y.size(0))).to(y.device)\n        s = out[idx, y]  # (batch, style_dim)\n        return s\n\n\nclass StyleEncoder(nn.Module):\n    def __init__(self, img_size=256, style_dim=64, num_domains=2, max_conv_dim=512):\n        super().__init__()\n        dim_in = 2**14 // img_size\n        blocks = []\n        blocks += [nn.Conv2d(3, dim_in, 3, 1, 1)]\n\n        repeat_num = int(np.log2(img_size)) - 2\n        for _ in range(repeat_num):\n            dim_out = min(dim_in*2, max_conv_dim)\n            blocks += [ResBlk(dim_in, dim_out, downsample=True)]\n            dim_in = dim_out\n\n        blocks += [nn.LeakyReLU(0.2)]\n        blocks += [nn.Conv2d(dim_out, dim_out, 4, 1, 0)]\n        blocks += [nn.LeakyReLU(0.2)]\n        self.shared = nn.Sequential(*blocks)\n\n        self.unshared = nn.ModuleList()\n        for _ in range(num_domains):\n            self.unshared += [nn.Linear(dim_out, style_dim)]\n\n    def forward(self, x, y):\n        h = self.shared(x)\n        h = h.view(h.size(0), -1)\n        out = []\n        for layer in self.unshared:\n            out += [layer(h)]\n        out = torch.stack(out, dim=1)  # (batch, num_domains, style_dim)\n        idx = torch.LongTensor(range(y.size(0))).to(y.device)\n        s = out[idx, y]  # (batch, style_dim)\n        return s\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, img_size=256, num_domains=2, max_conv_dim=512):\n        super().__init__()\n        dim_in = 2**14 // img_size\n        blocks = []\n        blocks += [nn.Conv2d(3, dim_in, 3, 1, 1)]\n\n        repeat_num = int(np.log2(img_size)) - 2\n        for _ in range(repeat_num):\n            dim_out = min(dim_in*2, max_conv_dim)\n            blocks += [ResBlk(dim_in, dim_out, downsample=True)]\n            dim_in = dim_out\n\n        blocks += [nn.LeakyReLU(0.2)]\n        blocks += [nn.Conv2d(dim_out, dim_out, 4, 1, 0)]\n        blocks += [nn.LeakyReLU(0.2)]\n        blocks += [nn.Conv2d(dim_out, num_domains, 1, 1, 0)]\n        self.main = nn.Sequential(*blocks)\n\n    def forward(self, x, y):\n        out = self.main(x)\n        out = out.view(out.size(0), -1)  # (batch, num_domains)\n        idx = torch.LongTensor(range(y.size(0))).to(y.device)\n        out = out[idx, y]  # (batch)\n        return out\n\n\ndef build_model(args):\n    generator = nn.DataParallel(Generator(args.img_size, args.style_dim, w_hpf=args.w_hpf))\n    mapping_network = nn.DataParallel(MappingNetwork(args.latent_dim, args.style_dim, args.num_domains))\n    style_encoder = nn.DataParallel(StyleEncoder(args.img_size, args.style_dim, args.num_domains))\n    discriminator = nn.DataParallel(Discriminator(args.img_size, args.num_domains))\n    generator_ema = copy.deepcopy(generator)\n    mapping_network_ema = copy.deepcopy(mapping_network)\n    style_encoder_ema = copy.deepcopy(style_encoder)\n\n    nets = Munch(generator=generator,\n                 mapping_network=mapping_network,\n                 style_encoder=style_encoder,\n                 discriminator=discriminator)\n    nets_ema = Munch(generator=generator_ema,\n                     mapping_network=mapping_network_ema,\n                     style_encoder=style_encoder_ema)\n\n    if args.w_hpf > 0:\n        fan = nn.DataParallel(FAN(fname_pretrained=args.wing_path).eval())\n        fan.get_heatmap = fan.module.get_heatmap\n        nets.fan = fan\n        nets_ema.fan = fan\n\n    return nets, nets_ema\n","metadata":{"execution":{"iopub.status.busy":"2023-01-04T10:05:38.324043Z","iopub.execute_input":"2023-01-04T10:05:38.324398Z","iopub.status.idle":"2023-01-04T10:05:38.389554Z","shell.execute_reply.started":"2023-01-04T10:05:38.324368Z","shell.execute_reply":"2023-01-04T10:05:38.388102Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nStarGAN v2\nCopyright (c) 2020-present NAVER Corp.\n\nThis work is licensed under the Creative Commons Attribution-NonCommercial\n4.0 International License. To view a copy of this license, visit\nhttp://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\nCreative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\"\"\"\n\nimport os\nfrom os.path import join as ospj\nimport time\nimport datetime\nfrom munch import Munch\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n\nclass Solver(nn.Module):\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n        self.nets, self.nets_ema = build_model(args)\n        # below setattrs are to make networks be children of Solver, e.g., for self.to(self.device)\n        for name, module in self.nets.items():\n            print_network(module, name)\n            setattr(self, name, module)\n        for name, module in self.nets_ema.items():\n            setattr(self, name + '_ema', module)\n\n        if args.mode == 'train':\n            self.optims = Munch()\n            for net in self.nets.keys():\n                if net == 'fan':\n                    continue\n                self.optims[net] = torch.optim.Adam(\n                    params=self.nets[net].parameters(),\n                    lr=args.f_lr if net == 'mapping_network' else args.lr,\n                    betas=[args.beta1, args.beta2],\n                    weight_decay=args.weight_decay)\n\n            self.ckptios = [\n                CheckpointIO(ospj(args.checkpoint_dir, '{:06d}_nets.ckpt'), data_parallel=True, **self.nets),\n                CheckpointIO(ospj(args.checkpoint_dir, '{:06d}_nets_ema.ckpt'), data_parallel=True, **self.nets_ema),\n                CheckpointIO(ospj(args.checkpoint_dir, '{:06d}_optims.ckpt'), **self.optims)]\n        else:\n            self.ckptios = [CheckpointIO(ospj(args.checkpoint_dir, '{:06d}_nets_ema.ckpt'), data_parallel=True, **self.nets_ema)]\n\n        self.to(self.device)\n        for name, network in self.named_children():\n            # Do not initialize the FAN parameters\n            if ('ema' not in name) and ('fan' not in name):\n                print('Initializing %s...' % name)\n                network.apply(he_init)\n\n    def _save_checkpoint(self, step):\n        for ckptio in self.ckptios:\n            ckptio.save(step)\n\n    def _load_checkpoint(self, step):\n        for ckptio in self.ckptios:\n            ckptio.load(step)\n\n    def _reset_grad(self):\n        for optim in self.optims.values():\n            optim.zero_grad()\n\n    def train(self, loaders):\n        args = self.args\n        nets = self.nets\n        nets_ema = self.nets_ema\n        optims = self.optims\n\n        # fetch random validation images for debugging\n        fetcher = InputFetcher(loaders.src, loaders.ref, args.latent_dim, 'train')\n        fetcher_val = InputFetcher(loaders.val, None, args.latent_dim, 'val')\n        inputs_val = next(fetcher_val)\n\n        # resume training if necessary\n        if args.resume_iter > 0:\n            self._load_checkpoint(args.resume_iter)\n\n        # remember the initial value of ds weight\n        initial_lambda_ds = args.lambda_ds\n\n        print('Start training...')\n        start_time = time.time()\n        for i in range(args.resume_iter, args.total_iters):\n            # fetch images and labels\n            inputs = next(fetcher)\n            x_real, y_org = inputs.x_src, inputs.y_src\n            x_ref, x_ref2, y_trg = inputs.x_ref, inputs.x_ref2, inputs.y_ref\n            z_trg, z_trg2 = inputs.z_trg, inputs.z_trg2\n\n            masks = nets.fan.get_heatmap(x_real) if args.w_hpf > 0 else None\n\n            # train the discriminator\n            d_loss, d_losses_latent = compute_d_loss(\n                nets, args, x_real, y_org, y_trg, z_trg=z_trg, masks=masks)\n            self._reset_grad()\n            d_loss.backward()\n            optims.discriminator.step()\n\n            d_loss, d_losses_ref = compute_d_loss(\n                nets, args, x_real, y_org, y_trg, x_ref=x_ref, masks=masks)\n            self._reset_grad()\n            d_loss.backward()\n            optims.discriminator.step()\n\n            # train the generator\n            g_loss, g_losses_latent = compute_g_loss(\n                nets, args, x_real, y_org, y_trg, z_trgs=[z_trg, z_trg2], masks=masks)\n            self._reset_grad()\n            g_loss.backward()\n            optims.generator.step()\n            optims.mapping_network.step()\n            optims.style_encoder.step()\n\n            g_loss, g_losses_ref = compute_g_loss(\n                nets, args, x_real, y_org, y_trg, x_refs=[x_ref, x_ref2], masks=masks)\n            self._reset_grad()\n            g_loss.backward()\n            optims.generator.step()\n\n            # compute moving average of network parameters\n            moving_average(nets.generator, nets_ema.generator, beta=0.999)\n            moving_average(nets.mapping_network, nets_ema.mapping_network, beta=0.999)\n            moving_average(nets.style_encoder, nets_ema.style_encoder, beta=0.999)\n\n            # decay weight for diversity sensitive loss\n            if args.lambda_ds > 0:\n                args.lambda_ds -= (initial_lambda_ds / args.ds_iter)\n\n            # print out log info\n            if (i+1) % args.print_every == 0:\n                elapsed = time.time() - start_time\n                elapsed = str(datetime.timedelta(seconds=elapsed))[:-7]\n                log = \"Elapsed time [%s], Iteration [%i/%i], \" % (elapsed, i+1, args.total_iters)\n                all_losses = dict()\n                for loss, prefix in zip([d_losses_latent, d_losses_ref, g_losses_latent, g_losses_ref],\n                                        ['D/latent_', 'D/ref_', 'G/latent_', 'G/ref_']):\n                    for key, value in loss.items():\n                        all_losses[prefix + key] = value\n                all_losses['G/lambda_ds'] = args.lambda_ds\n                log += ' '.join(['%s: [%.4f]' % (key, value) for key, value in all_losses.items()])\n                print(log)\n\n            # generate images for debugging\n            if (i+1) % args.sample_every == 0:\n                os.makedirs(args.sample_dir, exist_ok=True)\n                utils.debug_image(nets_ema, args, inputs=inputs_val, step=i+1)\n\n            # save model checkpoints\n            if (i+1) % args.save_every == 0:\n                self._save_checkpoint(step=i+1)\n\n            # compute FID and LPIPS if necessary\n            if (i+1) % args.eval_every == 0:\n                calculate_metrics(nets_ema, args, i+1, mode='latent')\n                calculate_metrics(nets_ema, args, i+1, mode='reference')\n\n    @torch.no_grad()\n    def sample(self, loaders):\n        args = self.args\n        nets_ema = self.nets_ema\n        os.makedirs(args.result_dir, exist_ok=True)\n        self._load_checkpoint(args.resume_iter)\n\n        src = next(InputFetcher(loaders.src, None, args.latent_dim, 'test'))\n        ref = next(InputFetcher(loaders.ref, None, args.latent_dim, 'test'))\n\n        fname = ospj(args.result_dir, 'reference.jpg')\n        print('Working on {}...'.format(fname))\n        utils.translate_using_reference(nets_ema, args, src.x, ref.x, ref.y, fname)\n\n        fname = ospj(args.result_dir, 'video_ref.mp4')\n        print('Working on {}...'.format(fname))\n        utils.video_ref(nets_ema, args, src.x, ref.x, ref.y, fname)\n\n    @torch.no_grad()\n    def evaluate(self):\n        args = self.args\n        nets_ema = self.nets_ema\n        resume_iter = args.resume_iter\n        self._load_checkpoint(args.resume_iter)\n        calculate_metrics(nets_ema, args, step=resume_iter, mode='latent')\n        calculate_metrics(nets_ema, args, step=resume_iter, mode='reference')\n\n\ndef compute_d_loss(nets, args, x_real, y_org, y_trg, z_trg=None, x_ref=None, masks=None):\n    assert (z_trg is None) != (x_ref is None)\n    # with real images\n    x_real.requires_grad_()\n    out = nets.discriminator(x_real, y_org)\n    loss_real = adv_loss(out, 1)\n    loss_reg = r1_reg(out, x_real)\n\n    # with fake images\n    with torch.no_grad():\n        if z_trg is not None:\n            s_trg = nets.mapping_network(z_trg, y_trg)\n        else:  # x_ref is not None\n            s_trg = nets.style_encoder(x_ref, y_trg)\n\n        x_fake = nets.generator(x_real, s_trg, masks=masks)\n    out = nets.discriminator(x_fake, y_trg)\n    loss_fake = adv_loss(out, 0)\n\n    loss = loss_real + loss_fake + args.lambda_reg * loss_reg\n    return loss, Munch(real=loss_real.item(),\n                       fake=loss_fake.item(),\n                       reg=loss_reg.item())\n\n\ndef compute_g_loss(nets, args, x_real, y_org, y_trg, z_trgs=None, x_refs=None, masks=None):\n    assert (z_trgs is None) != (x_refs is None)\n    if z_trgs is not None:\n        z_trg, z_trg2 = z_trgs\n    if x_refs is not None:\n        x_ref, x_ref2 = x_refs\n\n    # adversarial loss\n    if z_trgs is not None:\n        s_trg = nets.mapping_network(z_trg, y_trg)\n    else:\n        s_trg = nets.style_encoder(x_ref, y_trg)\n\n    x_fake = nets.generator(x_real, s_trg, masks=masks)\n    out = nets.discriminator(x_fake, y_trg)\n    loss_adv = adv_loss(out, 1)\n\n    # style reconstruction loss\n    s_pred = nets.style_encoder(x_fake, y_trg)\n    loss_sty = torch.mean(torch.abs(s_pred - s_trg))\n\n    # diversity sensitive loss\n    if z_trgs is not None:\n        s_trg2 = nets.mapping_network(z_trg2, y_trg)\n    else:\n        s_trg2 = nets.style_encoder(x_ref2, y_trg)\n    x_fake2 = nets.generator(x_real, s_trg2, masks=masks)\n    x_fake2 = x_fake2.detach()\n    loss_ds = torch.mean(torch.abs(x_fake - x_fake2))\n\n    # cycle-consistency loss\n    masks = nets.fan.get_heatmap(x_fake) if args.w_hpf > 0 else None\n    s_org = nets.style_encoder(x_real, y_org)\n    x_rec = nets.generator(x_fake, s_org, masks=masks)\n    loss_cyc = torch.mean(torch.abs(x_rec - x_real))\n\n    loss = loss_adv + args.lambda_sty * loss_sty \\\n        - args.lambda_ds * loss_ds + args.lambda_cyc * loss_cyc\n    return loss, Munch(adv=loss_adv.item(),\n                       sty=loss_sty.item(),\n                       ds=loss_ds.item(),\n                       cyc=loss_cyc.item())\n\n\ndef moving_average(model, model_test, beta=0.999):\n    for param, param_test in zip(model.parameters(), model_test.parameters()):\n        param_test.data = torch.lerp(param.data, param_test.data, beta)\n\n\ndef adv_loss(logits, target):\n    assert target in [1, 0]\n    targets = torch.full_like(logits, fill_value=target)\n    loss = F.binary_cross_entropy_with_logits(logits, targets)\n    return loss\n\n\ndef r1_reg(d_out, x_in):\n    # zero-centered gradient penalty for real images\n    batch_size = x_in.size(0)\n    grad_dout = torch.autograd.grad(\n        outputs=d_out.sum(), inputs=x_in,\n        create_graph=True, retain_graph=True, only_inputs=True\n    )[0]\n    grad_dout2 = grad_dout.pow(2)\n    assert(grad_dout2.size() == x_in.size())\n    reg = 0.5 * grad_dout2.view(batch_size, -1).sum(1).mean(0)\n    return reg","metadata":{"execution":{"iopub.status.busy":"2023-01-04T10:05:38.391623Z","iopub.execute_input":"2023-01-04T10:05:38.392148Z","iopub.status.idle":"2023-01-04T10:05:38.452827Z","shell.execute_reply.started":"2023-01-04T10:05:38.392107Z","shell.execute_reply":"2023-01-04T10:05:38.451434Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\n\nimport os\nimport argparse\n\nfrom munch import Munch\nfrom torch.backends import cudnn\nimport torch\n\n\ndef str2bool(v):\n    return v.lower() in ('true')\n\n\ndef subdirs(dname):\n    return [d for d in os.listdir(dname)\n            if os.path.isdir(os.path.join(dname, d))]\n\n\ndef main(args):\n    print(args)\n    cudnn.benchmark = True\n    torch.manual_seed(args.seed)\n\n    solver = Solver(args)\n\n    if args.mode == 'train':\n        assert len(subdirs(args.train_img_dir)) == args.num_domains\n        assert len(subdirs(args.val_img_dir)) == args.num_domains\n        loaders = Munch(src=get_train_loader(root=args.train_img_dir,\n                                             which='source',\n                                             img_size=args.img_size,\n                                             batch_size=args.batch_size,\n                                             prob=args.randcrop_prob,\n                                             num_workers=args.num_workers),\n                        ref=get_train_loader(root=args.train_img_dir,\n                                             which='reference',\n                                             img_size=args.img_size,\n                                             batch_size=args.batch_size,\n                                             prob=args.randcrop_prob,\n                                             num_workers=args.num_workers),\n                        val=get_test_loader(root=args.val_img_dir,\n                                            img_size=args.img_size,\n                                            batch_size=args.val_batch_size,\n                                            shuffle=True,\n                                            num_workers=args.num_workers))\n        solver.train(loaders)\n    elif args.mode == 'sample':\n        assert len(subdirs(args.src_dir)) == args.num_domains\n        assert len(subdirs(args.ref_dir)) == args.num_domains\n        loaders = Munch(src=get_test_loader(root=args.src_dir,\n                                            img_size=args.img_size,\n                                            batch_size=args.val_batch_size,\n                                            shuffle=False,\n                                            num_workers=args.num_workers),\n                        ref=get_test_loader(root=args.ref_dir,\n                                            img_size=args.img_size,\n                                            batch_size=args.val_batch_size,\n                                            shuffle=False,\n                                            num_workers=args.num_workers))\n        solver.sample(loaders)\n    elif args.mode == 'eval':\n        solver.evaluate()\n    elif args.mode == 'align':\n        from core.wing import align_faces\n        align_faces(args, args.inp_dir, args.out_dir)\n    else:\n        raise NotImplementedError\n\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser()\n\n    # model arguments\n    parser.add_argument('--img_size', type=int, default=256,\n                        help='Image resolution')\n    parser.add_argument('--num_domains', type=int, default=3,\n                        help='Number of domains')\n    parser.add_argument('--latent_dim', type=int, default=16,\n                        help='Latent vector dimension')\n    parser.add_argument('--hidden_dim', type=int, default=512,\n                        help='Hidden dimension of mapping network')\n    parser.add_argument('--style_dim', type=int, default=64,\n                        help='Style code dimension')\n\n    # weight for objective functions\n    parser.add_argument('--lambda_reg', type=float, default=1,\n                        help='Weight for R1 regularization')\n    parser.add_argument('--lambda_cyc', type=float, default=1,\n                        help='Weight for cyclic consistency loss')\n    parser.add_argument('--lambda_sty', type=float, default=1,\n                        help='Weight for style reconstruction loss')\n    parser.add_argument('--lambda_ds', type=float, default=1,\n                        help='Weight for diversity sensitive loss')\n    parser.add_argument('--ds_iter', type=int, default=100000,\n                        help='Number of iterations to optimize diversity sensitive loss')\n    parser.add_argument('--w_hpf', type=float, default=1,\n                        help='weight for high-pass filtering')\n\n    # training arguments\n    parser.add_argument('--randcrop_prob', type=float, default=0.5,\n                        help='Probabilty of using random-resized cropping')\n    parser.add_argument('--total_iters', type=int, default=100000,\n                        help='Number of total iterations')\n    parser.add_argument('--resume_iter', type=int, default=0,\n                        help='Iterations to resume training/testing')\n    parser.add_argument('--batch_size', type=int, default=8,\n                        help='Batch size for training')\n    parser.add_argument('--val_batch_size', type=int, default=32,\n                        help='Batch size for validation')\n    parser.add_argument('--lr', type=float, default=1e-4,\n                        help='Learning rate for D, E and G')\n    parser.add_argument('--f_lr', type=float, default=1e-6,\n                        help='Learning rate for F')\n    parser.add_argument('--beta1', type=float, default=0.0,\n                        help='Decay rate for 1st moment of Adam')\n    parser.add_argument('--beta2', type=float, default=0.99,\n                        help='Decay rate for 2nd moment of Adam')\n    parser.add_argument('--weight_decay', type=float, default=1e-4,\n                        help='Weight decay for optimizer')\n    parser.add_argument('--num_outs_per_domain', type=int, default=10,\n                        help='Number of generated images per domain during sampling')\n\n    # misc\n    parser.add_argument('--mode', type=str, default='train',\n                        choices=['train', 'sample', 'eval', 'align'],\n                        help='This argument is used in solver')\n    parser.add_argument('--num_workers', type=int, default=2,\n                        help='Number of workers used in DataLoader')\n    parser.add_argument('--seed', type=int, default=777,\n                        help='Seed for random number generator')\n\n    # directory for training\n    parser.add_argument('--train_img_dir', type=str, default='/kaggle/input/afhq-data/afhq/train',\n                        help='Directory containing training images')\n    parser.add_argument('--val_img_dir', type=str, default='/kaggle/input/afhq-data/afhq/val',\n                        help='Directory containing validation images')\n    parser.add_argument('--sample_dir', type=str, default='expr/samples',\n                        help='Directory for saving generated images')\n    parser.add_argument('--checkpoint_dir', type=str, default='expr/checkpoints',\n                        help='Directory for saving network checkpoints')\n\n    # directory for calculating metrics\n    parser.add_argument('--eval_dir', type=str, default='expr/eval',\n                        help='Directory for saving metrics, i.e., FID and LPIPS')\n\n    # directory for testing\n    parser.add_argument('--result_dir', type=str, default='expr/results',\n                        help='Directory for saving generated images and videos')\n    parser.add_argument('--src_dir', type=str, default='assets/representative/celeba_hq/src',\n                        help='Directory containing input source images')\n    parser.add_argument('--ref_dir', type=str, default='assets/representative/celeba_hq/ref',\n                        help='Directory containing input reference images')\n    parser.add_argument('--inp_dir', type=str, default='assets/representative/custom/female',\n                        help='input directory when aligning faces')\n    parser.add_argument('--out_dir', type=str, default='assets/representative/celeba_hq/src/female',\n                        help='output directory when aligning faces')\n\n    # face alignment\n    parser.add_argument('--wing_path', type=str, default='expr/checkpoints/wing.ckpt')\n    parser.add_argument('--lm_path', type=str, default='expr/checkpoints/celeba_lm_mean.npz')\n\n    # step size\n    parser.add_argument('--print_every', type=int, default=10)\n    parser.add_argument('--sample_every', type=int, default=5000)\n    parser.add_argument('--save_every', type=int, default=10000)\n    parser.add_argument('--eval_every', type=int, default=50000)\n\n    args = parser.parse_args(args=[])\n    main(args)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-04T10:07:40.045808Z","iopub.execute_input":"2023-01-04T10:07:40.046266Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Namespace(batch_size=8, beta1=0.0, beta2=0.99, checkpoint_dir='expr/checkpoints', ds_iter=100000, eval_dir='expr/eval', eval_every=50000, f_lr=1e-06, hidden_dim=512, img_size=256, inp_dir='assets/representative/custom/female', lambda_cyc=1, lambda_ds=1, lambda_reg=1, lambda_sty=1, latent_dim=16, lm_path='expr/checkpoints/celeba_lm_mean.npz', lr=0.0001, mode='train', num_domains=3, num_outs_per_domain=10, num_workers=2, out_dir='assets/representative/celeba_hq/src/female', print_every=10, randcrop_prob=0.5, ref_dir='assets/representative/celeba_hq/ref', result_dir='expr/results', resume_iter=0, sample_dir='expr/samples', sample_every=5000, save_every=10000, seed=777, src_dir='assets/representative/celeba_hq/src', style_dim=64, total_iters=100000, train_img_dir='/kaggle/input/afhq-data/afhq/train', val_batch_size=32, val_img_dir='/kaggle/input/afhq-data/afhq/val', w_hpf=1, weight_decay=0.0001, wing_path='expr/checkpoints/wing.ckpt')\nNumber of parameters of generator: 43467395\nNumber of parameters of mapping_network: 3259072\nNumber of parameters of style_encoder: 20949760\nNumber of parameters of discriminator: 20852803\nNumber of parameters of fan: 6333603\nInitializing generator...\nInitializing mapping_network...\nInitializing style_encoder...\nInitializing discriminator...\nPreparing DataLoader to fetch source images during the training phase...\nPreparing DataLoader to fetch reference images during the training phase...\nPreparing DataLoader for the generation phase...\nStart training...\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}